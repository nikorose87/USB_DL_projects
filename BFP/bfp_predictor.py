# -*- coding: utf-8 -*-
"""BFP_predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aH-49BIN5aHIK_bYoLF6msXEsuN4O2-P

# Body Fat Percentage Predictor (BFPP)

## Code description

In the following code process, a Body Fat Percentage Prediction is developed using kinematic marker trajectories on overground walking as input features. The dataset used for this prediction can be found in [Horst et al.](https://data.mendeley.com/datasets/svx74xcrjr/3).

The dataset used consists on kinematic trajectory information of 54 different body markers at each instance of gait cycle [from gait start (0%) to full gait (100%)] along 3 axes (X,Y,Z) for 57 subjects, with each subject performing 20$\pm$2 gait trials. 


Each gait trial information will be considered as a sample for the desgined regressor [rows], and its corresponding trajectory marker information as sample features [columns]. This means that input data array for the regressor will have:
* 57 subjects x 20$\pm$2 gait trials $\approx$ 1140 rows
* 100 gait cycle percentages x 54 markes x 3 axes = 16200 columns

The code is divided into three sections:
1. Input features preparation: In this part, the raw dataset is uploaded and processed in order to obtain input_features array as described above.
2. Target data preparation: Target body fat percentage is calculated for each sample using the equation developed by [Gallagher et al.](https://academic.oup.com/ajcn/article/72/3/694/4729363)
3. Body fat percentage prediction: A regressor is trained and tested using input_features and target_body_fat_percentage. Regressor performance is shown with a predicted values vs MSE per sample plot and Bland-Altman plot.

## Data 

In this study, the data from fifty-seven healthy and active subjects ($29$ females, $28$ males; $23.1 \pm 2.7$ years; $1.74 \pm 0.10$ m; $67.9 \pm 11.3$ kg) participated in the study. Each participant performed $20 \pm 2$ gait trials.In time domain dynamic information was stored in structured biomechanical formats (.tsv and .c3d) by trial.

We processed the data as follows: firstly, gait events (heel strike and toe off) were detected in the right limb by implementing the algorithms proposed in \cite{Zeni2008}. Then, We expressed each trial as a function of the gait cycle percentage (GC\%). A polynomial interpolation was performed over the time domain function in order to be adjusted to $m_i \in\mathbb{R}^{1\times 101}$ GC\% vector per $i$ trial. The coefficient of determination ($R^2$) of each interpolation was determined to evaluate the goodness of fit.
%
The resulting dataset was structured in a matrix $M_1 \in\mathbb{R}^{101 \times 57 \times 22\pm 2 \times 54 \times 3}$ as: rows represent the $101$ GC\%, and columns were multi-labeled in four levels by: the subject ID (from 1 to 57), the trial number, the marker label (a total of 54 markers) and coordinate axe (X, Y or Z), from the top to the bottom level, respectively. 

Finally, the data was rearranged in order to organize each trial with their corresponding features. In other words, a matrix $M_2 \in\mathbb{R}^{1142\times16362}$ was processed, placing trials as independent samples with their corresponding features that include: the 101 GC\% frames on each coordinate (X, Y and Z) for all 54 markers available. A total of 16362 characteristics (columns) as features were analyzed.  


## Importing libraries and donwloading the data
"""

import os
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.linear_model import LassoLarsCV
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.pipeline import make_pipeline, make_union, Pipeline
from tpot.builtins import StackingEstimator
from xgboost import XGBRegressor
from tpot.export_utils import set_param_recursive
from sklearn.metrics import mean_squared_error, r2_score
from skopt import BayesSearchCV
from skopt.space import Real
from matplotlib import pyplot as plt
import statsmodels.api as sm

def status_print(optim_result):
    """Status callback during bayesian hyperparameter search"""
    
    # Get all the models tested so far in DataFrame format
    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    
    
    # Get current parameters and the best parameters    
    best_params = pd.Series(bayes_cv_tuner.best_params_)
    print('Model #{}\nMin RMSE: {}\nBest params: {}\n'.format(
        len(all_models),
        np.round(bayes_cv_tuner.best_score_, 4),
        bayes_cv_tuner.best_params_
    ))
    
    # Save all model results
    clf_name = bayes_cv_tuner.estimator.__class__.__name__
    all_models.to_csv(clf_name+"_cv_results.csv")

#Loading the preprocessed dataset
root = os.getcwd()
os.chdir("/home/biomecanica4/Dropbox/Guardados/")
marker_data = pd.read_csv('Horst_markers_raw.csv',sep=',', header=[0,1], index_col=[0],
                          low_memory=False)
subject_info = pd.read_csv('Gait_subject_info.csv',sep=';', decimal=',', index_col=[0])
sublabels = ['Gender', 'Age', 'Body Mass', 'Height']
subject_info.columns = sublabels
os.chdir(root)


marker_data.head()

sublabels = ['Gender', 'Age', 'Body Mass', 'Height']
subject_info.columns = sublabels
subject_info.head()

#Fixing a mistake in labelling the level 1
marker_col_0 = list(marker_data.columns.get_level_values(0).unique())
new_marker_col_0 = ['{}_{}'.format(i,j) for i in marker_col_0 for j in ['X', 'Y', 'Z']]
marker_col_1 = list(marker_data.columns.get_level_values(1))
marker_col_1 = [i[:13] for i in marker_col_1]
marker_col_1 = set(marker_col_1)
new_cols = pd.MultiIndex.from_product([new_marker_col_0,list(marker_col_1)])
marker_data.columns = new_cols

#Letting trials being the samples

marker_data_new = marker_data.T
marker_data_new = marker_data_new.unstack(level=0)

#Reconstucting the index in order to establish the label
subjects = [i[:3] for i in marker_data_new.index]
trials = [i[5:8] for i in marker_data_new.index]
new_idx = pd.MultiIndex.from_arrays([subjects, trials])
marker_data_new.index = new_idx


for i in sublabels:
    marker_data_new[i] = 0

idx = pd.IndexSlice
for ind in subject_info.index:
    for num, j in enumerate(sublabels):
        marker_data_new.loc[idx[ind,:], j] = subject_info.loc[ind, subject_info.columns[num]]

marker_data_new['BMI'] = marker_data_new['Body Mass'] / np.power(marker_data_new['Height'],2)

#Formula to be applied BF% = 76.0 - 1097.8 * (1/BMI) - 20.6 * sex
       # + 0.053 * age + 154 * sex * (1/BMI)
       # + 0.034 * sex * age
      
#Replacing males as ones a females as 0
marker_data_new['Gender'] = marker_data_new['Gender'] -1
marker_data_new['Gender'] = marker_data_new['Gender'].replace(-1, 1)

BFP_formula = lambda age, bmi, sex: 76.0 - 1097.8 * (1/bmi) - 20.6 * \
    sex + 0.053 * age + 154 * sex * (1/bmi) + 0.034 * sex * age

marker_data_new['BFP'] = BFP_formula(marker_data_new.Age,
                                     marker_data_new.BMI,
                                     marker_data_new.Gender)

#Describe in the paper some data about max (32.12) and mins (9.40) in BFP

# =============================================================================
# Performing the ML model
# =============================================================================

marker_data_new = marker_data_new.dropna(axis=1)
#Mention that features are reduced from 16200 to 14300
marker_data_new

#Droppiing the sublabels
marker_data_new = marker_data_new.drop(sublabels, axis=1)

train_features, test_features, train_target, test_target = train_test_split(marker_data_new.iloc[:,:-1],
                                                                            marker_data_new['BFP'],
                                                    train_size=0.8, test_size=0.2, random_state=42)


#The base algorithm 
exported_pipeline = make_pipeline(
    PCA(iterated_power=2, svd_solver="randomized"),
    StackingEstimator(estimator=XGBRegressor(learning_rate=0.001, max_depth=5, 
                                             min_child_weight=11, n_estimators=100, 
                                             n_jobs=-1, objective="reg:squarederror", 
                                             subsample=1.0, verbosity=0)),
    LassoLarsCV(normalize=False))

set_param_recursive(exported_pipeline.steps, 'random_state', 0)

exported_pipeline.fit(train_features, train_target)

prediction = exported_pipeline.predict(test_features)
print("MSE = "+str(mean_squared_error(test_target, prediction)))
print("R2 = "+str(r2_score(test_target, prediction)))

# search over different model types
pipe = Pipeline([('PCA', PCA()), ('XGB', XGBRegressor(
        n_jobs = -1,
        objective = 'reg:squarederror',
        eval_metric = 'rmse',
        silent=1,
        tree_method='approx'
    ))])

# single categorical value of 'model' parameter is
# sets the model class
# We will get ConvergenceWarnings because the problem is not well-conditioned.
# But that's fine, this is just an example.
spaces = {
    'PCA__n_components': (2,10),
    'XGB__learning_rate': Real(low=0.01, high=1.0, prior='log-uniform'),
    'XGB__child_weight': (0, 10),
    'XGB__max_depth': (0, 50),
    'XGB__max_delta_step': (0, 20),
    'XGB__subsample': Real(low=0.01, high=1.0, prior='uniform'),
    'XGB__colsample_bytree': Real(low=0.01, high=1.0, prior='uniform'),
    'XGB__colsample_bylevel': Real(low=0.01, high=1.0, prior='uniform'),
    'XGB__reg_lambda': Real(low=1e-9, high=1000, prior='uniform'),
    'XGB__reg_alpha': Real(low=1e-9, high=1, prior='uniform'),
    'XGB__gamma': Real(low=1e-9, high=0.5, prior='log-uniform'),
    'XGB__min_child_weight': (0, 5),
    'XGB__n_estimators': (50, 100),
    'XGB__scale_pos_weight': Real(low=1e-6, high=500, prior='log-uniform') }

#Regressor
bayes_cv_tuner = BayesSearchCV(
    estimator = pipe, 
    search_spaces = spaces,
    scoring = 'neg_mean_squared_error',
    n_jobs = -1,
    n_iter = 100,   
    verbose = 0,
    refit = True,
    random_state = 42
)

  
# Fit the model
result = bayes_cv_tuner.fit(train_features.values, train_target.values, callback=status_print)


errors = list()
for i in range(len(prediction)):
    err = (test_target[i] - prediction[i])**2
    errors.append(err)
plt.scatter(prediction, errors)
plt.xlabel('Predicted Value')
plt.ylabel('Squared Error')
plt.title('Predicted values vs SE per sample')
plt.show()

test_target = test_target[:,np.newaxis]
prediction = prediction[:,np.newaxis]
errors = np.asarray(errors).reshape((227,1))
results_df = pd.DataFrame(data=np.hstack((test_target,prediction,errors)),columns=['Test target BF%', 'Predicted BF%', 'SE'])

f, ax = plt.subplots(1, figsize = (7,5))
sm.graphics.mean_diff_plot(test_target, prediction, ax = ax)
plt.title('Mean difference plot')
plt.show()

